# Benchmarking tensor network contractions

Device information:
1. NVIDIA A100-PCIE 80G with NVIDIA Driver Version 470.82.01 and CUDA Version 11.4
2. NVIDIA V100-SXM2 16G with NVIDIA Driver Version 470.63.01 and CUDA Version 11.4
3. Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz

Contraction order `data/tensornetwork_permutation_optimized.json` is generated by the following code (you do not need to run this, because we have put the contraction order in the `data/` directory)
```julia
julia> using OMEinsum, OMEinsumContractionOrders, Graphs

julia> function random_regular_eincode(n, k; optimize=nothing)
            g = Graphs.random_regular_graph(n, k)
            ixs = [minmax(e.src,e.dst) for e in Graphs.edges(g)]
            return EinCode((ixs..., [(i,) for i in     Graphs.vertices(g)]...), ())
           end
random_regular_eincode (generic function with 1 method)

julia> code = random_regular_eincode(220, 3);

julia> optcode_tree = optimize_code(code, uniformsize(code, 2), TreeSA(sc_target=29, Î²s=0.1:0.1:20,
                                                             ntrials=5, niters=30, sc_weight=2.0));

julia> timespace_complexity(optcode_tree, uniformsize(code, 2))
(33.17598124621909, 28.0)

julia> writejson("tensornetwork_permutation_optimized.json", optcode_tree)
```

## Setup

* Install [uv](https://github.com/astral-sh/uv) for Python dependency management
* Install [Julia](https://julialang.org/downloads/)

```bash
$ make init
```

This will initialize both Julia and Python environments. You can also initialize them separately:
- `make init-julia` - Initialize Julia environment (in `julia/` directory)
- `make init-python` - Initialize Python environment via uv (in `python/` directory)

(NOTE: if you want to update your local environment, just run `make update`)

## Quick Start - Using Makefile

The easiest way to run benchmarks and compare results is using the Makefile:

```bash
# Run all benchmarks (PyTorch + OMEinsum with all backends)
$ make run-all

# Generate summary and PDF report with plots
$ make summary
$ make report
```

Available benchmark targets:
- `make run-pytorch-gpu` - Run PyTorch GPU benchmark
- `make run-pytorch-cpu` - Run PyTorch CPU benchmark
- `make run-julia-gpu` - Run OMEinsum GPU benchmark (CUBLAS backend)
- `make run-julia-cpu` - Run OMEinsum CPU benchmark
- `make run-julia-cutensor` - Run OMEinsum GPU benchmark (cuTENSOR backend)

Report generation (requires [Typst](https://typst.app/)):
- `make report` - Generate PDF report with comparison plots
- `make figures` - Generate SVG figures

Customizable parameters:
- `DEVICE_ID=0` - CUDA device ID
- `REPEAT=10` - Number of repetitions
- `TENSORNETWORK=tensornetwork_permutation_optimized.json` - Tensor network file

Example with custom parameters:
```bash
$ make run-julia-gpu DEVICE_ID=1 REPEAT=20
```

Results are saved to JSON files in the `results/` directory. The `summary` target generates `results/summary.json`, and the `report` target creates a PDF report with comparison plots using Typst.

To see all available targets:
```bash
$ make help
```

## Manual Execution

### pytorch

```bash
$ cd python
$ uv run benchmark_pytorch.py gpu
$ uv run benchmark_pytorch.py cpu
```

#### Timing

* on A100, the minimum time is ~0.12s, 10 execusions take ~1.35s
* on V100, the minimum time is ~0.12s, 10 execusions take ~1.76s
* on CPU (MKL backend, single thread), it is 38.87s (maybe MKL is not set properly?)

### OMEinsum.jl

```bash
$ cd julia
$ JULIA_NUM_THREADS=1 julia --project benchmark_OMEinsum.jl gpu
$ JULIA_NUM_THREADS=1 julia --project benchmark_OMEinsum.jl cpu
```

#### Using cuTENSOR backend

To benchmark with NVIDIA's cuTENSOR library for optimized tensor contractions:

```bash
$ cd julia
$ JULIA_NUM_THREADS=1 julia --project benchmark_OMEinsum.jl gpu --backend=cutensor
```

Note: cuTENSOR provides optimized kernels for binary tensor contractions and may offer better performance for certain contraction patterns.

#### Comparing backends

To directly compare CUBLAS (default) vs cuTENSOR backends:

```bash
$ cd julia
$ JULIA_NUM_THREADS=1 julia --project compare_backends.jl compare
```

This will run both backends on the same tensor network and report the speedup.

#### Timing
* on A100, the minimum time is ~0.16s, 10 execusions take ~2.25s
* on V100, the minimum time is ~0.13s, 10 execusions take ~1.39s
* on CPU (MKL backend, single thread), it is 23.05s

Note: The Julia garbadge collection time is avoided.

## Results

All benchmark results are saved as JSON files in the `results/` directory:
- `pytorch_gpu_<deviceid>.json` - PyTorch GPU results
- `pytorch_cpu.json` - PyTorch CPU results
- `julia_gpu_cublas_<deviceid>.json` - OMEinsum GPU results (CUBLAS backend)
- `julia_gpu_cutensor_<deviceid>.json` - OMEinsum GPU results (cuTENSOR backend)
- `julia_cpu.json` - OMEinsum CPU results
- `summary.json` - Consolidated summary of all results

The summary includes timing information (min, median, mean, total) and metadata for each benchmark run.

### Generated Reports

After running benchmarks, you can generate visual reports:
- `report.pdf` - PDF report with comparison plots and tables (generated by `make report`)
- `figures/benchmark_comparison.svg` - SVG figure for embedding (generated by `make figures`)

The reports are generated using [Typst](https://typst.app/) and include:
- Bar charts comparing execution times across frameworks and backends
- Tables with speedup calculations relative to PyTorch
- GPU vs CPU performance comparison


## Notes
The python scripts are contributed by @Fanerst, there are other people in the discussion and provide helpful advices, please check the [original post](https://github.com/under-Peter/OMEinsum.jl/issues/133#issuecomment-1003662057).
